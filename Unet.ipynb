{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPg6vjmVSPbfuhan6EqHWMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YonedaRyuki/UNet/blob/main/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBq-Rwfp8l-4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9SfdTtV8wdm"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kbfMclf83WB"
      },
      "source": [
        "class Down(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, out_channels),\n",
        "    )\n",
        "  def forward(x):\n",
        "    self.maxpool_conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Torj3D3R4MO"
      },
      "source": [
        "class Up(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "    super().__init__()\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode ='bilinear', align_corners=True)\n",
        "      self.conv = DoubleConv(in_channels, out_channels, in_channels //2)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels, in_channels //2, kernel_size=2, stride=2)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x2.size()[3]\n",
        "\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                    diffY // 2, diffY - diffY // 2])\n",
        "    x = torch.cat([x2,x1],dim = 1)\n",
        "    return self.conv(x)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNlurtrIVqOp"
      },
      "source": [
        "class OutConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(OutConv, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdKPXOZUgtRU"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZhGLUjCaLpi"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy7LFtPTeCca"
      },
      "source": [
        "net = UNet(1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8MLPSJSd7S8",
        "outputId": "0f5cf4db-01ff-4f3a-bd0d-2d0b2c7f225d"
      },
      "source": [
        "summary(net,(1,475,475))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 475, 475]             640\n",
            "       BatchNorm2d-2         [-1, 64, 475, 475]             128\n",
            "              ReLU-3         [-1, 64, 475, 475]               0\n",
            "            Conv2d-4         [-1, 64, 475, 475]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 475, 475]             128\n",
            "              ReLU-6         [-1, 64, 475, 475]               0\n",
            "        DoubleConv-7         [-1, 64, 475, 475]               0\n",
            "         MaxPool2d-8         [-1, 64, 237, 237]               0\n",
            "            Conv2d-9        [-1, 128, 237, 237]          73,856\n",
            "      BatchNorm2d-10        [-1, 128, 237, 237]             256\n",
            "             ReLU-11        [-1, 128, 237, 237]               0\n",
            "           Conv2d-12        [-1, 128, 237, 237]         147,584\n",
            "      BatchNorm2d-13        [-1, 128, 237, 237]             256\n",
            "             ReLU-14        [-1, 128, 237, 237]               0\n",
            "       DoubleConv-15        [-1, 128, 237, 237]               0\n",
            "             Down-16        [-1, 128, 237, 237]               0\n",
            "        MaxPool2d-17        [-1, 128, 118, 118]               0\n",
            "           Conv2d-18        [-1, 256, 118, 118]         295,168\n",
            "      BatchNorm2d-19        [-1, 256, 118, 118]             512\n",
            "             ReLU-20        [-1, 256, 118, 118]               0\n",
            "           Conv2d-21        [-1, 256, 118, 118]         590,080\n",
            "      BatchNorm2d-22        [-1, 256, 118, 118]             512\n",
            "             ReLU-23        [-1, 256, 118, 118]               0\n",
            "       DoubleConv-24        [-1, 256, 118, 118]               0\n",
            "             Down-25        [-1, 256, 118, 118]               0\n",
            "        MaxPool2d-26          [-1, 256, 59, 59]               0\n",
            "           Conv2d-27          [-1, 512, 59, 59]       1,180,160\n",
            "      BatchNorm2d-28          [-1, 512, 59, 59]           1,024\n",
            "             ReLU-29          [-1, 512, 59, 59]               0\n",
            "           Conv2d-30          [-1, 512, 59, 59]       2,359,808\n",
            "      BatchNorm2d-31          [-1, 512, 59, 59]           1,024\n",
            "             ReLU-32          [-1, 512, 59, 59]               0\n",
            "       DoubleConv-33          [-1, 512, 59, 59]               0\n",
            "             Down-34          [-1, 512, 59, 59]               0\n",
            "        MaxPool2d-35          [-1, 512, 29, 29]               0\n",
            "           Conv2d-36          [-1, 512, 29, 29]       2,359,808\n",
            "      BatchNorm2d-37          [-1, 512, 29, 29]           1,024\n",
            "             ReLU-38          [-1, 512, 29, 29]               0\n",
            "           Conv2d-39          [-1, 512, 29, 29]       2,359,808\n",
            "      BatchNorm2d-40          [-1, 512, 29, 29]           1,024\n",
            "             ReLU-41          [-1, 512, 29, 29]               0\n",
            "       DoubleConv-42          [-1, 512, 29, 29]               0\n",
            "             Down-43          [-1, 512, 29, 29]               0\n",
            "         Upsample-44          [-1, 512, 58, 58]               0\n",
            "           Conv2d-45          [-1, 512, 59, 59]       4,719,104\n",
            "      BatchNorm2d-46          [-1, 512, 59, 59]           1,024\n",
            "             ReLU-47          [-1, 512, 59, 59]               0\n",
            "           Conv2d-48          [-1, 256, 59, 59]       1,179,904\n",
            "      BatchNorm2d-49          [-1, 256, 59, 59]             512\n",
            "             ReLU-50          [-1, 256, 59, 59]               0\n",
            "       DoubleConv-51          [-1, 256, 59, 59]               0\n",
            "               Up-52          [-1, 256, 59, 59]               0\n",
            "         Upsample-53        [-1, 256, 118, 118]               0\n",
            "           Conv2d-54        [-1, 256, 118, 118]       1,179,904\n",
            "      BatchNorm2d-55        [-1, 256, 118, 118]             512\n",
            "             ReLU-56        [-1, 256, 118, 118]               0\n",
            "           Conv2d-57        [-1, 128, 118, 118]         295,040\n",
            "      BatchNorm2d-58        [-1, 128, 118, 118]             256\n",
            "             ReLU-59        [-1, 128, 118, 118]               0\n",
            "       DoubleConv-60        [-1, 128, 118, 118]               0\n",
            "               Up-61        [-1, 128, 118, 118]               0\n",
            "         Upsample-62        [-1, 128, 236, 236]               0\n",
            "           Conv2d-63        [-1, 128, 237, 237]         295,040\n",
            "      BatchNorm2d-64        [-1, 128, 237, 237]             256\n",
            "             ReLU-65        [-1, 128, 237, 237]               0\n",
            "           Conv2d-66         [-1, 64, 237, 237]          73,792\n",
            "      BatchNorm2d-67         [-1, 64, 237, 237]             128\n",
            "             ReLU-68         [-1, 64, 237, 237]               0\n",
            "       DoubleConv-69         [-1, 64, 237, 237]               0\n",
            "               Up-70         [-1, 64, 237, 237]               0\n",
            "         Upsample-71         [-1, 64, 474, 474]               0\n",
            "           Conv2d-72         [-1, 64, 475, 475]          73,792\n",
            "      BatchNorm2d-73         [-1, 64, 475, 475]             128\n",
            "             ReLU-74         [-1, 64, 475, 475]               0\n",
            "           Conv2d-75         [-1, 64, 475, 475]          36,928\n",
            "      BatchNorm2d-76         [-1, 64, 475, 475]             128\n",
            "             ReLU-77         [-1, 64, 475, 475]               0\n",
            "       DoubleConv-78         [-1, 64, 475, 475]               0\n",
            "               Up-79         [-1, 64, 475, 475]               0\n",
            "           Conv2d-80          [-1, 2, 475, 475]             130\n",
            "          OutConv-81          [-1, 2, 475, 475]               0\n",
            "================================================================\n",
            "Total params: 17,266,306\n",
            "Trainable params: 17,266,306\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.86\n",
            "Forward/backward pass size (MB): 3232.45\n",
            "Params size (MB): 65.87\n",
            "Estimated Total Size (MB): 3299.17\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_iPHIm0enCR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}